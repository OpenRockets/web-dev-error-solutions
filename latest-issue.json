[{"body":"\nThis document addresses a common issue developers encounter when storing large text posts (e.g., blog posts, articles) in Firebase Firestore: exceeding the document size limit.  Firestore has a limit on the size of a single document, and exceeding this limit results in errors during write operations.\n\n**Description of the Error:**\n\nWhen attempting to store a large post exceeding Firestore's document size limit (currently 1 MB), you will encounter an error similar to this:\n\n```\nError: Document size exceeds the maximum allowed size (1 MiB).\n```\n\nThis error prevents the entire post from being saved to Firestore.  This is especially problematic with rich text posts containing images or embedded media.\n\n**Fixing Step-by-Step (Code):**\n\nThis solution involves splitting the post content into smaller, manageable chunks and storing them as separate subcollections within a main post document.\n\n**1. Data Structure:**\n\nWe'll use a structure where each post has a main document storing metadata, and a subcollection to hold the post's content in smaller chunks.\n\n\n```json\n// Post document\n{\n  \"postId\": \"post123\",\n  \"title\": \"My Awesome Blog Post\",\n  \"author\": \"John Doe\",\n  \"createdAt\": 1678886400000,\n  \"contentChunks\": [ // Array of chunk references\n      {\n          \"chunkId\": \"chunk1\",\n          \"order\": 1\n      },\n      {\n          \"chunkId\": \"chunk2\",\n          \"order\": 2\n      }\n  ]\n}\n\n// Subcollection \"content\" (Documents within the post document)\n// chunk1\n{\n  \"chunkId\": \"chunk1\",\n  \"content\": \"This is the first part of the blog post...\"\n}\n// chunk2\n{\n  \"chunkId\": \"chunk2\",\n  \"content\": \"This is the second part of the blog post...\"\n}\n\n```\n\n\n**2. JavaScript Code (using Firebase Admin SDK):**\n\n\n```javascript\nconst admin = require('firebase-admin');\nadmin.initializeApp();\nconst db = admin.firestore();\n\nasync function storeLargePost(postId, title, author, content) {\n  const chunkSize = 5000; // Adjust as needed.  Smaller chunks mean more overhead, larger means higher risk of exceeding limits.\n  const chunks = [];\n  const contentChunksRefs = [];\n\n  let currentChunk = \"\";\n  for (let i = 0; i < content.length; i++) {\n    currentChunk += content[i];\n    if (currentChunk.length >= chunkSize) {\n      const chunkId = `chunk${chunks.length + 1}`;\n      const chunkRef = db.collection(\"posts\").doc(postId).collection(\"content\").doc(chunkId);\n      await chunkRef.set({ chunkId, content: currentChunk });\n      chunks.push({chunkId, order: chunks.length + 1}); // keep track of references\n      contentChunksRefs.push({chunkId, order: chunks.length});\n      currentChunk = \"\";\n    }\n  }\n\n  // Add the last chunk if any\n  if (currentChunk.length > 0) {\n    const chunkId = `chunk${chunks.length + 1}`;\n    const chunkRef = db.collection(\"posts\").doc(postId).collection(\"content\").doc(chunkId);\n    await chunkRef.set({ chunkId, content: currentChunk });\n    chunks.push({chunkId, order: chunks.length + 1});\n    contentChunksRefs.push({chunkId, order: chunks.length});\n  }\n\n\n  const postRef = db.collection(\"posts\").doc(postId);\n  await postRef.set({\n    postId,\n    title,\n    author,\n    createdAt: admin.firestore.FieldValue.serverTimestamp(),\n    contentChunks: contentChunksRefs\n  });\n\n  console.log(\"Post stored successfully!\");\n}\n\n\n// Example usage:\nconst longPostContent = \"This is a very long blog post that exceeds the Firestore document size limit.  This is a very long blog post that exceeds the Firestore document size limit.  This is a very long blog post that exceeds the Firestore document size limit.  This is a very long blog post that exceeds the Firestore document size limit.  This is a very long blog post that exceeds the Firestore document size limit. \";\n\n\nstoreLargePost(\"post123\", \"My Long Post\", \"Jane Doe\", longPostContent).catch(err => console.error(\"Error storing post:\", err));\n\n```\n\n\n**3. Retrieving the Post:**\n\nYou'll need to retrieve the chunks and concatenate them on the client-side:\n\n\n```javascript\n//Client side (e.g., using Firebase web SDK)\n\nasync function getPost(postId){\n    const postDoc = await db.collection('posts').doc(postId).get();\n    const postData = postDoc.data();\n    let fullContent = '';\n\n    for (const chunkRef of postData.contentChunks){\n        const chunkDoc = await db.collection('posts').doc(postId).collection('content').doc(chunkRef.chunkId).get();\n        fullContent += chunkDoc.data().content;\n    }\n    return { ...postData, fullContent};\n}\n\n```\n\n\n**Explanation:**\n\nThe code divides the post content into smaller chunks of a specified size (`chunkSize`).  Each chunk is stored as a separate document in a subcollection. The main post document then contains references (ids) to these chunks, allowing retrieval and reassembly.  This approach avoids exceeding the document size limit.\n\n\n**External References:**\n\n* [Firestore Data Size Limits](https://firebase.google.com/docs/firestore/quotas)\n* [Firebase Admin SDK](https://firebase.google.com/docs/admin/setup)\n* [Firebase Web SDK](https://firebase.google.com/docs/web/setup)\n\n\nCopyrights (c) OpenRockets Open-source Network. Free to use, copy, share, edit or publish.\n","number":2711,"title":"Handling Firestore Data Limits When Storing Large Posts"}]
